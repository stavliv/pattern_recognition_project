{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Training and Evaluation\n",
    "This notebook solves the D part of the assignemnt by training a NN using PyTorch. \n",
    "It includes data preprocessing, model definition, training, cross-validation, and test set prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "The following libraries are imported to facilitate data processing, model creation, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "The training and test datasets are loaded and preprocessed. Features are scaled, and the labels are converted to a format suitable for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load and preprocess train and test datasets.\"\"\"\n",
    "    \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_path=\"datasetTV.csv\"\n",
    "test_path=\"datasetTest.csv\"\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "X = train_data.iloc[:, :-1].values\n",
    "y = train_data.iloc[:, -1].values\n",
    "X_test = test_data.values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "y = torch.tensor(y - 1, dtype=torch.long).to(DEVICE) # 0-indexed\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for class imbalance in the dataset\n",
    "We check for class imbalance in the dataset, so we know if it is something we have to take care when continuing with the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts: Counter({np.int64(4): 1784, np.int64(0): 1768, np.int64(2): 1754, np.int64(1): 1720, np.int64(3): 1716})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check for class imbalance\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Count the number of samples for each class\n",
    "class_counts = Counter(y.cpu().numpy())\n",
    "print(\"Class Counts:\", class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are fairly balanced in the dataset, so we do not have to take any measures regarding class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the NN Architecture and functions that we will need for training and plotting.\n",
    "* A Neural Network Classifier with 2 hidden layers is chosen, ReLu is our activation function, and we also implement dropout layers to prevent overfitting and boost generalization.\n",
    "* The `train_model` function trains a model for a specified number of epochs, given the desired optimizer, criteria, and the training and validation datasets. It aslo allows for `patience` implementation, to enable early stopping when the validation losses start to plateu. *Note: By implementing this function we reduce code duplication, since we train various models in the rest of our code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, h1, h2, num_classes):\n",
    "        \"\"\"Fully connected neural network with batch normalization and dropout.\"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, h1)\n",
    "        self.bn1 = nn.BatchNorm1d(h1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.bn2 = nn.BatchNorm1d(h2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.out = nn.Linear(h2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    \"\"\"Plot the training and validation loss curves.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criteria, epochs, device=DEVICE, patience=None):\n",
    "    \"\"\"Train and evaluate the model for the given number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model (): PyTorch model.\n",
    "        train_loader: PyTorch DataLoader for training set.\n",
    "        val_loader: PyTorch DataLoader for validation set.\n",
    "        optimizer: PyTorch optimizer.\n",
    "        criteria: Loss function.\n",
    "        epochs: Number of epochs to train the model.\n",
    "        device: Device to run the model on. Default on `DEVICE`.\n",
    "        patience: Number of epochs to wait before early stopping if validation loss does not decrease. Default is None.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses: List of training losses for each epoch\n",
    "        val_losses: List of validation losses for each epoch\n",
    "    \n",
    "    \"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "\n",
    "        # Training step\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criteria(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss, num_samples, correct_predicted = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for val_X, val_y in val_loader:\n",
    "                val_X, val_y = val_X.to(device), val_y.to(device)\n",
    "                outputs = model(val_X)\n",
    "                val_loss += criteria(outputs, val_y).item()\n",
    "                correct_predicted += accuracy_score(\n",
    "                    outputs.argmax(dim=1).cpu().numpy(), val_y.cpu().numpy(), normalize=False\n",
    "                )\n",
    "                num_samples += val_y.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Accuracy: {correct_predicted / num_samples:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Early stopping\n",
    "        if patience:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "                    break\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation\n",
    "We train with the **k-fold cross validation** technique.Meaning \n",
    "\n",
    "* we create *k* datasets, and on each one of them a differents part of the whole training set provided, will be split into *validation set* and *training set*, \n",
    "* and *k* models will be trained.\n",
    "\n",
    "That way we can keep track of the performance of our approach (especially overfitting) in an as much as possible unbiased way, since we will see the **validatiion vs training loss** for different validation sets, and the results will not be depended on a single validation set. If we used a single validation set without performing cross validation there is a chance that the single validation set will happen to be very similar to the training set, and our ability to judge our model's generaliaztion abilty will be biased.\n",
    "\n",
    "### Hyper parameter Grid Search\n",
    "The code supports grid-search for the hyper parameters. After performing the grid-search we saw that almost all parameters give similar results, but approach minimiaztion of loss at different number of epochs.\n",
    "\n",
    "Our chosen final parametrs are:\n",
    "* learning rate = 0.001\n",
    "* batch size = 64\n",
    "* Hidden layers dimension = 128 (for both hidden layers)\n",
    "\n",
    "If you wish to perform the grid-search you can just uncomment the definition of the grid search parameters.\n",
    "\n",
    "## Choices:\n",
    "* k = 5 (meaning 1/5 folds, so 20% of our data are validation set each time)\n",
    "* Cross-entropy loss (good for clasification)\n",
    "* Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Perform K-fold cross-validation for hyperparameter tuning.\"\"\"\n",
    "\n",
    "train_params = {\n",
    "    \"learning_rate\": [0.0001],\n",
    "    \"batch_size\": [64],\n",
    "    \"hidden_layer_size\": [128],\n",
    "    \"epochs\": 100,\n",
    "    # Uncomment to test multiple hyperparameters, and comment the corresping lines above\n",
    "    # \"learning_rate\": [0.0001, 0.001, 0.01],\n",
    "    # \"batch_size\": [32, 64, 128],\n",
    "    # \"hidden_layer_size\": [32, 64, 128],\n",
    "}\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "for lr, bs, hls in product(\n",
    "    train_params[\"learning_rate\"], train_params[\"batch_size\"], train_params[\"hidden_layer_size\"]\n",
    "):\n",
    "    fold_val_losses = []\n",
    "    print(f\"Testing Hyperparameters: lr: {lr}, bs: {bs}, hls: {hls}\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(x)):\n",
    "        print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "        model = Model(in_features=224, h1=hls, h2=hls, num_classes=5).to(DEVICE)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        train_losses, val_losses = train_model(\n",
    "            model, train_loader, val_loader, optimizer, criteria, train_params[\"epochs\"]\n",
    "        )\n",
    "\n",
    "        fold_val_losses.append(val_losses[-1])\n",
    "\n",
    "    avg_val_loss = sum(fold_val_losses) / n_folds\n",
    "    print(f\"Average Validation Loss for lr={lr}, bs={bs}, hls={hls}: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the final model\n",
    "After peforming cross-validation training to examine closer the behaviour of our system, we decided that we don't want to waste 20% of the data for validation, so we chose to train our final model on 90% of the data, using only 10% for validation. \n",
    "\n",
    "That way we still have a good information on the prorgress of overfitting, without loosing a big amount of our data. \n",
    "\n",
    "Using 20% on cross-validation seem a good value to get an unbiased opinion about our hyperparameters and the architecture of our model.\n",
    "\n",
    "So we train the final model without cross-fold validation, and with 10% of the training data as validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the final model with a small validation set.\"\"\"\n",
    "\n",
    "train_params = {\n",
    "    \"learning_rate\": [0.0001],\n",
    "    \"batch_size\": [64],\n",
    "    \"hidden_layer_size\": [128],\n",
    "    \"epochs\": 100,\n",
    "}\n",
    "\n",
    "# Split into train and pseudo-validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = train_params[\"batch_size\"][0]\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = Model(\n",
    "    in_features=224,\n",
    "    h1=train_params[\"hidden_layer_size\"][0],\n",
    "    h2=train_params[\"hidden_layer_size\"][0],\n",
    "    num_classes=5\n",
    ").to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=train_params[\"learning_rate\"][0])\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, optimizer, criteria, train_params[\"epochs\"], patience=10\n",
    ")\n",
    "\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on Test Data\n",
    "The trained model is used to predict the labels for the test dataset, and the results are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Predict test data.\"\"\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    _, predicted_labels = torch.max(test_outputs, 1)\n",
    "    predicted_labels += 1  # Convert back to 1-based indexing\n",
    "\n",
    "np.save(\"labelsX.npy\", predicted_labels.cpu().numpy())\n",
    "print(\"Predictions saved as labelsX.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sure labels can be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 2 ... 3 2 1] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Make sure that the labelsX.npy file can be read.\"\"\"\n",
    "\n",
    "labels = np.load(\"labelsX.npy\")\n",
    "print(labels, type(labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
